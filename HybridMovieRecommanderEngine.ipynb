{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CagataySencan/Hybrid-Movie-Recommender-System/blob/main/HybridMovieRecommanderEngine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ1wLfS4iuGk"
      },
      "source": [
        "# Genel Hazırlıklar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv65UC4uKgqX"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy as stats\n",
        "from ast import literal_eval\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
        "import nltk \n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from collections import Counter\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import wordnet\n",
        "!pip install scikit-surprise\n",
        "from scipy.sparse import csr_matrix\n",
        "from surprise import Reader,Dataset,SVD\n",
        "from surprise.model_selection import cross_validate\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "!pip install fuzzywuzzy\n",
        "from fuzzywuzzy import process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCayD8VrIfhz"
      },
      "outputs": [],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ug743yU_I9Uy"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UG9-pxr-I_1g"
      },
      "outputs": [],
      "source": [
        "!cp /content/kaggle.json ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Tn9ts8oiJGIK"
      },
      "outputs": [],
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mE8xqPPJHoN"
      },
      "outputs": [],
      "source": [
        "! kaggle datasets download rounakbanik/the-movies-dataset\n",
        "! kaggle datasets download tmdb/tmdb-movie-metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrA6mlG8JVdo"
      },
      "outputs": [],
      "source": [
        "! unzip /content/the-movies-dataset.zip\n",
        "! unzip /content/tmdb-movie-metadata.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5Nmr04izKI0H"
      },
      "outputs": [],
      "source": [
        "credits = pd.DataFrame(pd.read_csv('credits.csv'))\n",
        "keywords = pd.DataFrame(pd.read_csv('keywords.csv'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxF-AvSjilwU"
      },
      "source": [
        "# Content Based Filtering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peLUEfwX5bsh"
      },
      "outputs": [],
      "source": [
        "# Content based filtering için hazırlık\n",
        "movies_metadata_tmdb = pd.DataFrame(pd.read_csv('tmdb_5000_movies.csv'))\n",
        "movies_tmdb_credits = pd.DataFrame(pd.read_csv('tmdb_5000_credits.csv'))\n",
        "content_based_movies = movies_metadata_tmdb[['overview','title','genres', 'keywords']]\n",
        "content_based_credits = movies_tmdb_credits[['cast','crew','movie_id']]\n",
        "content_based = content_based_movies.join(content_based_credits)\n",
        "# Yeterli veri olduğu için eksik veri olan satırları çıkartma kararı aldım\n",
        "content_based = content_based.dropna()\n",
        "\n",
        "# Bu filtreden en yüksek verimi alabilmek için iki alt filtreye bölme kararı aldım\n",
        "# İlk filtre önerileri overview'a, ikincisi ise cast, crew, keyword, genre gibi parametrelere göre öneri yapacak\n",
        "# Bu filtrede memory yetersizliği nedeniyle cosine similarity score hesaplayamadığım için TMDB 5000 Movie Dataset'i kullandım\n",
        "content_based.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5QQ8Zjdi3fi"
      },
      "source": [
        "### Overview Bazlı Filtre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5jKm550idj_"
      },
      "outputs": [],
      "source": [
        "# overview bazlı filtre :\n",
        "\n",
        "overview_based = content_based[['title','overview']]\n",
        "print(overview_based.isnull().sum())\n",
        "print(overview_based.head())\n",
        "\n",
        "# Vektörleştirme işlemi \n",
        "\n",
        "tf_idf = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix_overview = tf_idf.fit_transform(overview_based['overview'])\n",
        "\n",
        "\n",
        "# Filmler arasındaki benzerlikleri nümerik bir skorla görmek için 'cosine similarity score' hesabı yapma kararı aldım\n",
        "tfidf_matrix_overview.shape\n",
        "similarity_overview = linear_kernel(tfidf_matrix_overview,tfidf_matrix_overview)\n",
        "overview_based = overview_based.reset_index()\n",
        "index = pd.Series(overview_based.index, index=overview_based['title']).drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8LT8uE9NghzS"
      },
      "outputs": [],
      "source": [
        "# Girilen film ismine göre öneri yapacak olan fonksiyon :\n",
        "\n",
        "def recommend_by_overview(title, sim = similarity_overview) :\n",
        "  idx = index[title]\n",
        "  sim_score = list(enumerate(sim[idx]))\n",
        "  sim_score = sorted(sim_score, key=lambda x: x[1], reverse=True)\n",
        "  sim_score = sim_score[1:11]\n",
        "  movie_index = [i[0] for i in sim_score]\n",
        "  recommendation = overview_based['title'].iloc[movie_index].tolist()\n",
        "  recDict = {}\n",
        "  i = 0\n",
        "  while i < 10 :\n",
        "    recDict[recommendation[i]] = sim_score[i][1]\n",
        "    i += 1\n",
        "\n",
        "  return recDict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDKc9wlgjD6z"
      },
      "source": [
        "### Cast, Crew, Keyword, Genre Bazlı Filtre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhQMiaPFiXVZ"
      },
      "outputs": [],
      "source": [
        "features = ['cast', 'crew', 'keywords', 'genres']\n",
        "feature_based = content_based[['title','cast', 'crew', 'keywords', 'genres']]\n",
        "\n",
        "for feature in features : \n",
        "  feature_based[feature] = feature_based[feature].apply(literal_eval)\n",
        "\n",
        "# Yönetmenleri crew sütunundan almak için gerekli fonksiyon  \n",
        "def add_director(j) :\n",
        "  for i in j :\n",
        "    if i['job'] == 'Director':\n",
        "      return i['name']\n",
        "  return np.nan\n",
        "\n",
        "# Diğer sütunlardaki bilgilerden ilk 5 tanesini almak için gerekli fonksiyon\n",
        "def add_list(j) :\n",
        "  if isinstance(j, list):\n",
        "        \n",
        "        names = [i['name'] for i in j]\n",
        "        \n",
        "        if len(names) > 5:\n",
        "            names = names[:5]\n",
        "        return names\n",
        "   \n",
        "  return []       \n",
        "\n",
        "# Veriyi kullanabilmek için uyumlu forma getirme işlemi\n",
        "feature_based['director'] = feature_based['crew'].apply(add_director)\n",
        "features = ['cast', 'keywords', 'genres']\n",
        "\n",
        "for feature in features:\n",
        "    feature_based[feature] = feature_based[feature].apply(add_list)\n",
        "\n",
        "feature_based = feature_based.drop(columns = ['crew'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "z-G5nSKY_VXU"
      },
      "outputs": [],
      "source": [
        "# Bütun satırlardaki verileri küçük harfe dönüştürme ve boşlukları düzenleme işlemi \n",
        "def clean(j):\n",
        "    if isinstance(j, list):\n",
        "        return [str.lower(i.replace(\" \", \"\")) for i in j]\n",
        "    else:\n",
        "        if isinstance(j, str):\n",
        "            return str.lower(j.replace(\" \", \"\"))\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "features = ['cast', 'keywords', 'director', 'genres']\n",
        "\n",
        "for feature in features:\n",
        "    feature_based[feature] = feature_based[feature].apply(clean)\n",
        "\n",
        "# Vektörleştirme işlemini tek seferde yapabilmek için bütün verileri içeren tek bir sütun oluşturma işlemi\n",
        "def all_data(x):\n",
        "    return ' '.join(x['keywords']) + ' ' + ' '.join(x['cast']) + ' ' + x['director'] + ' ' + ' '.join(x['genres'])\n",
        "\n",
        "\n",
        "feature_based['all'] = feature_based.apply(all_data, axis=1)\n",
        "\n",
        "tf_idf = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix_feature = tf_idf.fit_transform(feature_based['all'])\n",
        "\n",
        "# Filmler arasındaki benzerlikleri nümerik bir skorla görmek için 'cosine similarity score' hesabı yapma kararı aldım\n",
        "tfidf_matrix_feature.shape\n",
        "similarity_feature = linear_kernel(tfidf_matrix_feature,tfidf_matrix_feature)\n",
        "feature_based = feature_based.reset_index()\n",
        "index = pd.Series(feature_based.index, index=feature_based['title']).drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "c-7VcMKRUpzK"
      },
      "outputs": [],
      "source": [
        "def recommend_by_feature(title, sim = similarity_feature) :\n",
        "  idx = index[title]\n",
        "  sim_score = list(enumerate(sim[idx]))\n",
        "  sim_score = sorted(sim_score, key=lambda x: x[1], reverse=True)\n",
        "  sim_score = sim_score[1:11]\n",
        "  movie_index = [i[0] for i in sim_score]\n",
        "  recommendation = feature_based['title'].iloc[movie_index].tolist()\n",
        "  recDict = {}\n",
        "  i = 0\n",
        "  while i < 10 :\n",
        "    recDict[recommendation[i]] = sim_score[i][1]\n",
        "    i += 1\n",
        "\n",
        "  return recDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eUPOCSLF2wD"
      },
      "outputs": [],
      "source": [
        "# İki filtreden de gelen veriler dictionary şeklinde olduğundan dolayı değerden anahtar kelimeyi bulan bir fonksiyona ihtiyacım oldu\n",
        "def get_key(d, val):\n",
        "    keys = [k for k, v in d.items() if v == val]\n",
        "    if keys:\n",
        "        return keys[0]\n",
        "    return None\n",
        "\n",
        "# Buradaki fonksiyonda iki filtreden gelen cosine similarity score değeri bir listeye atanıyor \n",
        "# Liste büyükten küçüğe sıralanıp değeri en büyük olan 10 film öneri olarak veriliyor\n",
        "def final_recommendation_content_based(title) :\n",
        "  list1 = recommend_by_overview(title)\n",
        "  list2 = recommend_by_feature(title)\n",
        "  value_list = list(list1.values()) + list(list2.values())\n",
        "  value_list.sort(reverse = True)\n",
        "  recommend_list = []\n",
        "  i = 0 \n",
        "  while i < 20 :\n",
        "    key = get_key(list1,value_list[i])\n",
        "    if key is None :\n",
        "      new_key = get_key(list2,value_list[i])\n",
        "      if new_key in recommend_list :\n",
        "        i += 1 \n",
        "        continue\n",
        "      else :\n",
        "        recommend_list.append(new_key)    \n",
        "    else :\n",
        "      if key in recommend_list :\n",
        "        i += 1 \n",
        "        continue\n",
        "      else :\n",
        "        recommend_list.append(key)  \n",
        "    i += 1  \n",
        "  recommend_list = recommend_list[0:10]\n",
        "  return recommend_list\n",
        "\n",
        "final_recommendation_content_based('The Godfather')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP1UCSdqVz8a"
      },
      "source": [
        "# Collaborative Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### User-Based Collaborative Filtering"
      ],
      "metadata": {
        "id": "3_YDk_3GynSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtre için veri hazırlığı\n",
        "movies = pd.read_csv('movies_metadata.csv')\n",
        "ratings = pd.read_csv('ratings_uploaded.csv')\n",
        "movies = movies[['id','title']]\n",
        "movies['movieId'] = movies['id']\n",
        "movies = movies.drop('id', axis = 1)  "
      ],
      "metadata": {
        "id": "pTpXJ0wiDzHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SVD ile kullanıcıya ve girdiği filme göre vereceği rating'i tahmin etme işlemi \n",
        "def collaborative_filter(movie_id,user_id) : \n",
        "  reader = Reader()\n",
        "\n",
        "  data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
        "\n",
        "  svd = SVD()\n",
        "  cross_validate(svd, data, measures=['RMSE', 'MAE'])\n",
        "  trainset = data.build_full_trainset()\n",
        "  svd.fit(trainset)\n",
        "\n",
        "  score = str(svd.predict(2000, movie_id, 3.5))\n",
        "  score = score.split()\n",
        "  return score[9]"
      ],
      "metadata": {
        "id": "-ByTEZlZMjCM"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demographic Filtering"
      ],
      "metadata": {
        "id": "9-4zTlt2Yzn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bu filtreleme çeşidi proje içerisinde pek önemli bir yer arz etmiyor. Tamamen popülerlik bazlı çalışmaktadır.\n",
        "movies_metadata = pd.read_csv('movies_metadata.csv')\n",
        "vote_average= movies_metadata['vote_average'].mean()\n",
        "vote_count= movies_metadata['vote_count'].quantile(0.9)\n",
        "\n",
        "demo_movies = movies_metadata.copy().loc[movies_metadata['vote_count'] >= vote_count]\n",
        "\n",
        "# IMDB'nin film değerlendirmesi için kullandığı formülü uygulayalım\n",
        "def weighted_rating(x, vote_count=vote_count, vote_average=vote_average):\n",
        "    vote_numbers = x['vote_count']\n",
        "    average = x['vote_average']\n",
        "    # Calculation based on the IMDB formula\n",
        "    return (vote_numbers/(vote_numbers+vote_count) * average) + (vote_count/(vote_count+vote_numbers) * vote_average)\n",
        "\n",
        "def demographic_recommender(demo_movies = demo_movies) : \n",
        "  demo_movies['score'] = demo_movies.apply(weighted_rating, axis=1)\n",
        "  demo_movies = demo_movies.sort_values('score', ascending=False)\n",
        "  return demo_movies['title'].head(3).tolist()    "
      ],
      "metadata": {
        "id": "K1D5R_UWY4mT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Review Based Filtering"
      ],
      "metadata": {
        "id": "4Y22eOOERqV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = pd.read_csv('reviews.csv')\n",
        "reviews = reviews.drop('id', axis = 1)\n",
        "\n",
        "# Text temizleme işlemleri \n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "print(reviews['sentiment'].value_counts())\n",
        "reviews['review'] = reviews['review'].astype(str)\n",
        "for i in range(reviews['review'].count()) : \n",
        "   \n",
        "   text = str(reviews['review'][i])\n",
        "   text = text.lower()\n",
        "   text = remove_stopwords(text)\n",
        "   text = text.translate(str.maketrans('','', punctuation))\n",
        "   reviews['review'][i] = text\n",
        "\n",
        "# Common wordlerden temizleme\n",
        "cnt = Counter()\n",
        "for text in reviews['review'].values : \n",
        "  for word in text.split() :\n",
        "    cnt[word] += 1\n",
        "\n",
        "\n",
        "freq = set([w for (w, wc) in cnt.most_common(10)])\n",
        "def removeFreqwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in freq])\n",
        "\n",
        "reviews['review'] = reviews['review'].apply(lambda text: removeFreqwords(text))\n",
        "reviews.head()\n",
        "\n",
        "# Lemmalama işlemi\n",
        "lm = WordNetLemmatizer()\n",
        "\n",
        "def lemmatizer(text):\n",
        "    return \" \".join([lm.lemmatize(word) for word in text.split()])\n",
        "\n",
        "reviews['review'] = reviews['review'].apply(lambda text: lemmatizer(text))\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "reviews['scores'] = reviews['review'].apply(lambda review: sia.polarity_scores(review))\n",
        "\n",
        "# Polarity score analizi için compound isimli bir sütun oluşturma işlemi\n",
        "reviews['compound'] = reviews['scores'].apply(lambda comp: comp['compound'])\n",
        "\n",
        "# Polarity score'a göre duyguları yeniden düzenleme\n",
        "reviews['comp_score'] = reviews['compound'].apply(lambda c: 1 if c >= 0 else 0)\n",
        "print(reviews.head)\n",
        "reviews = reviews.drop(['review','sentiment','scores','compound'],axis = 1)\n",
        "print(reviews['comp_score'].value_counts())"
      ],
      "metadata": {
        "id": "k5h_FyB7Rth2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def review_based_recommender(movie_name):\n",
        "  values = list(reviews['comp_score'].value_counts()) \n",
        "  mov_values = list(reviews.loc[reviews.title == movie_name,'comp_score'].values)\n",
        "  positive_count = mov_values.count(1)\n",
        "  negative_count = mov_values.count(0)\n",
        "  total_count = positive_count + negative_count\n",
        "  percent_positive = round((positive_count*100)/(total_count))\n",
        "  percent_positive_text = \"%\" + str(percent_positive) + \" of reviews are positive\"\n",
        "  return percent_positive_text\n",
        "\n",
        "x = review_based_recommender('The Lost World: Jurassic Park')\n",
        "x  "
      ],
      "metadata": {
        "id": "2IBGiEo7dRUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hybrid Recommender"
      ],
      "metadata": {
        "id": "xpIYx3pyvwkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def final_recommendation(movie_name,user_id):\n",
        "  content_rec = final_recommendation_content_based(movie_name)\n",
        "  demo_rec = demographic_recommender()\n",
        "  review_rec = review_based_recommender(movie_name)\n",
        "  new_list = list(set(content_rec + demo_rec))\n",
        "  \n",
        "  collab_evalution = []\n",
        "  collab_evalution_id = []\n",
        "  id_list = []\n",
        "  name_list = []\n",
        "  review_list = []\n",
        "  review_list_temp = []\n",
        "\n",
        "  for i in new_list :\n",
        "    isin = movies.loc[movies['title'] == i]\n",
        "    isin = list(isin['title'])\n",
        "    for i in isin:\n",
        "      if(i not in name_list ) :\n",
        "        name_list.append(i)   \n",
        "  for j in name_list:\n",
        "    id = movies.loc[movies['title'] == j]\n",
        "    id = list(id['id'])\n",
        "    id_list.append(id)  \n",
        "  for i in id_list :\n",
        "      collab_evalution_id.append(i[0])    \n",
        "  for i in collab_evalution_id :\n",
        "    collab_evalution.append(collaborative_filter(int(i),user_id))\n",
        "  for i in name_list :\n",
        "    isin = reviews.loc[reviews['title'] == i]\n",
        "    isin = list(isin['title'])\n",
        "    for j in isin :\n",
        "      if j not in review_list_temp:\n",
        "        review_list_temp.append(j)\n",
        "           \n",
        "\n",
        "  for i in name_list:\n",
        "    if i in review_list_temp:\n",
        "      review_list.append(review_based_recommender(i))\n",
        "    else:\n",
        "      review_list.append(\"There is not any review for this movie\")  \n",
        "\n",
        "  data = {'Movie Name' : name_list,\n",
        "         'Estimated Rating for User 2000' : collab_evalution,\n",
        "         'Reviews by Other Users' : review_list}\n",
        "  final_frame = pd.DataFrame(data)\n",
        "  final_frame = final_frame.sort_values(by=['Estimated Rating for User 2000'],ascending=False)\n",
        "  return final_frame.head(10)\n",
        "\n",
        "recommandation = final_recommendation(\"Interstellar\",2000)\n",
        "recommandation          "
      ],
      "metadata": {
        "id": "SfX-l7ALA_KK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "HybridMovieRecommanderEngine.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMXCM7h8i+/Xsp5MugvYYzM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}