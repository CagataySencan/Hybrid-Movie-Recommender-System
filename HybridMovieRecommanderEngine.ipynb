{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CagataySencan/Hybrid-Movie-Recommender-System/blob/main/HybridMovieRecommanderEngine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Genel Hazırlıklar"
      ],
      "metadata": {
        "id": "ZZ1wLfS4iuGk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv65UC4uKgqX"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy as stats\n",
        "from ast import literal_eval\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "!pip install scikit-surprise\n",
        "from scipy.sparse import csr_matrix\n",
        "from surprise import Reader,Dataset,SVD\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "!pip install fuzzywuzzy\n",
        "from fuzzywuzzy import process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCayD8VrIfhz"
      },
      "outputs": [],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ug743yU_I9Uy"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UG9-pxr-I_1g"
      },
      "outputs": [],
      "source": [
        "!cp /content/kaggle.json ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Tn9ts8oiJGIK"
      },
      "outputs": [],
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mE8xqPPJHoN"
      },
      "outputs": [],
      "source": [
        "! kaggle datasets download rounakbanik/the-movies-dataset\n",
        "! kaggle datasets download tmdb/tmdb-movie-metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrA6mlG8JVdo"
      },
      "outputs": [],
      "source": [
        "! unzip /content/the-movies-dataset.zip\n",
        "! unzip /content/tmdb-movie-metadata.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5Nmr04izKI0H"
      },
      "outputs": [],
      "source": [
        "credits = pd.DataFrame(pd.read_csv('credits.csv'))\n",
        "keywords = pd.DataFrame(pd.read_csv('keywords.csv'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Content Based Filtering\n"
      ],
      "metadata": {
        "id": "GxF-AvSjilwU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peLUEfwX5bsh"
      },
      "outputs": [],
      "source": [
        "# Content based filtering için hazırlık\n",
        "movies_metadata_tmdb = pd.DataFrame(pd.read_csv('tmdb_5000_movies.csv'))\n",
        "movies_tmdb_credits = pd.DataFrame(pd.read_csv('tmdb_5000_credits.csv'))\n",
        "content_based_movies = movies_metadata_tmdb[['overview','title','genres', 'keywords']]\n",
        "content_based_credits = movies_tmdb_credits[['cast','crew','movie_id']]\n",
        "content_based = content_based_movies.join(content_based_credits)\n",
        "# Yeterli veri olduğu için eksik veri olan satırları çıkartma kararı aldım\n",
        "content_based = content_based.dropna()\n",
        "\n",
        "# Bu filtreden en yüksek verimi alabilmek için iki alt filtreye bölme kararı aldım\n",
        "# İlk filtre önerileri overview'a, ikincisi ise cast, crew, keyword, genre gibi parametrelere göre öneri yapacak\n",
        "# Bu filtrede memory yetersizliği nedeniyle cosine similarity score hesaplayamadığım için TMDB 5000 Movie Dataset'i kullandım\n",
        "content_based.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overview Bazlı Filtre"
      ],
      "metadata": {
        "id": "G5QQ8Zjdi3fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# overview bazlı filtre :\n",
        "\n",
        "overview_based = content_based[['title','overview']]\n",
        "print(overview_based.isnull().sum())\n",
        "print(overview_based.head())\n",
        "\n",
        "# Vektörleştirme işlemi \n",
        "\n",
        "tf_idf = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix_overview = tf_idf.fit_transform(overview_based['overview'])\n",
        "\n",
        "\n",
        "# Filmler arasındaki benzerlikleri nümerik bir skorla görmek için 'cosine similarity score' hesabı yapma kararı aldım\n",
        "tfidf_matrix_overview.shape\n",
        "similarity_overview = linear_kernel(tfidf_matrix_overview,tfidf_matrix_overview)\n",
        "overview_based = overview_based.reset_index()\n",
        "index = pd.Series(overview_based.index, index=overview_based['title']).drop_duplicates()"
      ],
      "metadata": {
        "id": "d5jKm550idj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Girilen film ismine göre öneri yapacak olan fonksiyon :\n",
        "\n",
        "def recommend_by_overview(title, sim = similarity_overview) :\n",
        "  idx = index[title]\n",
        "  sim_score = list(enumerate(sim[idx]))\n",
        "  sim_score = sorted(sim_score, key=lambda x: x[1], reverse=True)\n",
        "  sim_score = sim_score[1:11]\n",
        "  movie_index = [i[0] for i in sim_score]\n",
        "  recommendation = overview_based['title'].iloc[movie_index].tolist()\n",
        "  recDict = {}\n",
        "  i = 0\n",
        "  while i < 10 :\n",
        "    recDict[recommendation[i]] = sim_score[i][1]\n",
        "    i += 1\n",
        "\n",
        "  return recDict"
      ],
      "metadata": {
        "id": "8LT8uE9NghzS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cast, Crew, Keyword, Genre Bazlı Filtre"
      ],
      "metadata": {
        "id": "jDKc9wlgjD6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['cast', 'crew', 'keywords', 'genres']\n",
        "feature_based = content_based[['title','cast', 'crew', 'keywords', 'genres']]\n",
        "\n",
        "for feature in features : \n",
        "  feature_based[feature] = feature_based[feature].apply(literal_eval)\n",
        "\n",
        "# Yönetmenleri crew sütunundan almak için gerekli fonksiyon  \n",
        "def add_director(j) :\n",
        "  for i in j :\n",
        "    if i['job'] == 'Director':\n",
        "      return i['name']\n",
        "  return np.nan\n",
        "\n",
        "# Diğer sütunlardaki bilgilerden ilk 5 tanesini almak için gerekli fonksiyon\n",
        "def add_list(j) :\n",
        "  if isinstance(j, list):\n",
        "        \n",
        "        names = [i['name'] for i in j]\n",
        "        \n",
        "        if len(names) > 5:\n",
        "            names = names[:5]\n",
        "        return names\n",
        "   \n",
        "  return []       \n",
        "\n",
        "# Veriyi kullanabilmek için uyumlu forma getirme işlemi\n",
        "feature_based['director'] = feature_based['crew'].apply(add_director)\n",
        "features = ['cast', 'keywords', 'genres']\n",
        "\n",
        "for feature in features:\n",
        "    feature_based[feature] = feature_based[feature].apply(add_list)\n",
        "\n",
        "feature_based = feature_based.drop(columns = ['crew'])"
      ],
      "metadata": {
        "id": "fhQMiaPFiXVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bütun satırlardaki verileri küçük harfe dönüştürme ve boşlukları düzenleme işlemi \n",
        "def clean(j):\n",
        "    if isinstance(j, list):\n",
        "        return [str.lower(i.replace(\" \", \"\")) for i in j]\n",
        "    else:\n",
        "        if isinstance(j, str):\n",
        "            return str.lower(j.replace(\" \", \"\"))\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "features = ['cast', 'keywords', 'director', 'genres']\n",
        "\n",
        "for feature in features:\n",
        "    feature_based[feature] = feature_based[feature].apply(clean)\n",
        "\n",
        "# Vektörleştirme işlemini tek seferde yapabilmek için bütün verileri içeren tek bir sütun oluşturma işlemi\n",
        "def all_data(x):\n",
        "    return ' '.join(x['keywords']) + ' ' + ' '.join(x['cast']) + ' ' + x['director'] + ' ' + ' '.join(x['genres'])\n",
        "\n",
        "\n",
        "feature_based['all'] = feature_based.apply(all_data, axis=1)\n",
        "\n",
        "tf_idf = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix_feature = tf_idf.fit_transform(feature_based['all'])\n",
        "\n",
        "# Filmler arasındaki benzerlikleri nümerik bir skorla görmek için 'cosine similarity score' hesabı yapma kararı aldım\n",
        "tfidf_matrix_feature.shape\n",
        "similarity_feature = linear_kernel(tfidf_matrix_feature,tfidf_matrix_feature)\n",
        "feature_based = feature_based.reset_index()\n",
        "index = pd.Series(feature_based.index, index=feature_based['title']).drop_duplicates()"
      ],
      "metadata": {
        "id": "z-G5nSKY_VXU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_by_feature(title, sim = similarity_feature) :\n",
        "  idx = index[title]\n",
        "  sim_score = list(enumerate(sim[idx]))\n",
        "  sim_score = sorted(sim_score, key=lambda x: x[1], reverse=True)\n",
        "  sim_score = sim_score[1:11]\n",
        "  movie_index = [i[0] for i in sim_score]\n",
        "  recommendation = feature_based['title'].iloc[movie_index].tolist()\n",
        "  recDict = {}\n",
        "  i = 0\n",
        "  while i < 10 :\n",
        "    recDict[recommendation[i]] = sim_score[i][1]\n",
        "    i += 1\n",
        "\n",
        "  return recDict"
      ],
      "metadata": {
        "id": "c-7VcMKRUpzK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# İki filtreden de gelen veriler dictionary şeklinde olduğundan dolayı değerden anahtar kelimeyi bulan bir fonksiyona ihtiyacım oldu\n",
        "def get_key(d, val):\n",
        "    keys = [k for k, v in d.items() if v == val]\n",
        "    if keys:\n",
        "        return keys[0]\n",
        "    return None\n",
        "\n",
        "# Buradaki fonksiyonda iki filtreden gelen cosine similarity score değeri bir listeye atanıyor \n",
        "# Liste büyükten küçüğe sıralanıp değeri en büyük olan 10 film öneri olarak veriliyor\n",
        "def final_recommendation_content_based(title) :\n",
        "  list1 = recommend_by_overview(title)\n",
        "  list2 = recommend_by_feature(title)\n",
        "  value_list = list(list1.values()) + list(list2.values())\n",
        "  value_list.sort(reverse = True)\n",
        "  recommend_list = []\n",
        "  i = 0 \n",
        "  while i < 20 :\n",
        "    key = get_key(list1,value_list[i])\n",
        "    if key is None :\n",
        "      new_key = get_key(list2,value_list[i])\n",
        "      if new_key in recommend_list :\n",
        "        i += 1 \n",
        "        continue\n",
        "      else :\n",
        "        recommend_list.append(new_key)    \n",
        "    else :\n",
        "      if key in recommend_list :\n",
        "        i += 1 \n",
        "        continue\n",
        "      else :\n",
        "        recommend_list.append(key)  \n",
        "    i += 1  \n",
        "  recommend_list = recommend_list[0:10]\n",
        "  return recommend_list\n",
        "\n",
        "final_recommendation_content_based('The Godfather')"
      ],
      "metadata": {
        "id": "6eUPOCSLF2wD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "109b9c8d-b879-4f68-dbd1-5983811c7ee0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The Godfather: Part II',\n",
              " 'The Godfather: Part III',\n",
              " 'Apocalypse Now',\n",
              " 'Blood Ties',\n",
              " 'This Thing of Ours',\n",
              " 'The Apostle',\n",
              " 'All Good Things',\n",
              " 'Easy Money',\n",
              " 'Black Mass',\n",
              " 'Made']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collaborative Filtering"
      ],
      "metadata": {
        "id": "AP1UCSdqVz8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "movies_metadata = pd.DataFrame(pd.read_csv('movies_metadata.csv'))\n",
        "ratings = pd.DataFrame(pd.read_csv('ratings_small.csv'))\n",
        "\n",
        "movies = movies_metadata[['id','title']]\n",
        "movies['movieId'] = movies['id']\n",
        "movies = movies.drop('id',axis = 1)\n",
        "ratings = ratings.drop('timestamp', axis=1)\n",
        "\n",
        "user_nums = len(ratings.userId.unique())\n",
        "item_nums = len(movies.movieId.unique())\n",
        "\n",
        "total_count = user_nums * item_nums\n",
        "\n",
        "rating_zero = total_count - ratings.shape[0]\n",
        "\n",
        "ratings_count_temp = pd.DataFrame(ratings.groupby('rating').size(), columns = ['count'])\n",
        "ratings_count = ratings_count_temp.append(pd.DataFrame({'count': rating_zero}, index=[0.0]),verify_integrity=True,).sort_index()\n",
        "ratings_count['log_count'] = np.log(ratings_count['count'])\n",
        "\n",
        "# Rating frekansı öğrenme işlemleri\n",
        "movies_count = pd.DataFrame(ratings.groupby('movieId').size(), columns = ['count'])\n",
        "movies_count.head()\n",
        "movies_count['count'].quantile(np.arange(1, 0.6, -0.05))\n",
        "\n",
        "# Popüler olmayıp veriyi bozacak filmlerin çıkarılması\n",
        "popular_movies = list(set(movies_count.query('count >= 50').index))\n",
        "popular_ratings = ratings[ratings.movieId.isin(popular_movies)]\n",
        "\n",
        "# User başına oylama sayısı\n",
        "users_count = pd.DataFrame(popular_ratings.groupby('userId').size(), columns=['count'])\n",
        "users_count['count'].quantile(np.arange(1, 0.5, -0.05))\n",
        "\n",
        "# Kayda değer oy vermeyen kullanıcıların çıkarılması \n",
        "active_users = list(set(users_count.query('count >= 50').index))\n",
        "popular_active_ratings = popular_ratings[popular_ratings.userId.isin(active_users)]\n",
        "# Movie-User matrisi\n",
        "movie_user_mat = popular_active_ratings.pivot(index='movieId', columns='userId', values='rating').fillna(0)\n",
        "movie_to_idx = {\n",
        "    movies: i for i, movies in \n",
        "    enumerate(list(movies.set_index('movieId').loc[movie_user_mat.index].title))\n",
        "}\n",
        "# KNN modeline parametre olarak vermek için sparse matrisi oluşturma işlemi\n",
        "movie_user_mat_sparse = csr_matrix(movie_user_mat.values)"
      ],
      "metadata": {
        "id": "qIHqooVBV7Vm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "HybridMovieRecommanderEngine.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPxnLAGzn7eZ7TwMjnM37WK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}